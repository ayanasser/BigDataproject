{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b82ac7f-5e2a-448a-b546-7fe13860de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff, lag, when, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8606131e-a8bd-4e58-9002-92bf9b4e1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadmissionRiskPrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a79b3a4-73d1-442e-9c93-5a803cf6d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ADMISSIONS.csv file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"hdfs://namenode:9000/mimic-iii/ADMISSIONS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b632f54-8a4f-4476-bf5d-267041ee71d9",
   "metadata": {},
   "source": [
    "# Part I. Readmission Risk Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2274d1-f55e-4a64-b329-6c4f42dc3443",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e656b77-f9c9-49c5-83b4-d2eeb44a23fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+-------------------+-----------+\n",
      "|SUBJECT_ID|HADM_ID|LENGTH_OF_STAY|NUM_PREV_ADMISSIONS|READMISSION|\n",
      "+----------+-------+--------------+-------------------+-----------+\n",
      "|      2322| 181695|            15|                  2|          1|\n",
      "|      3242| 175206|             7|                  2|          1|\n",
      "|      3490| 135086|             0|                  1|          1|\n",
      "|      3792| 132278|             0|                  3|          1|\n",
      "|      4155| 124456|             2|                  3|          1|\n",
      "|      5768| 164070|             1|                  1|          1|\n",
      "|      5897| 137321|            10|                  1|          1|\n",
      "|      7184| 134761|             1|                  1|          1|\n",
      "|      7666| 159952|            46|                  8|          1|\n",
      "|      8389| 122962|             7|                  1|          1|\n",
      "|      8426| 142053|             9|                  2|          1|\n",
      "|      8452| 175505|             1|                  4|          1|\n",
      "|      8674| 158403|             9|                  3|          1|\n",
      "|      8698| 188386|            19|                  6|          1|\n",
      "|      9575| 190574|            12|                  2|          1|\n",
      "|     11578| 115245|             4|                  0|          0|\n",
      "|     12008| 187452|            16|                  4|          1|\n",
      "|     12310| 185464|             2|                  1|          1|\n",
      "|     12530| 107003|             5|                  2|          1|\n",
      "|     12799| 120131|            36|                  2|          1|\n",
      "+----------+-------+--------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert ADMITTIME and DISCHTIME to timestamps\n",
    "df = df.withColumn(\"ADMITTIME\", col(\"ADMITTIME\").cast(\"timestamp\")) \\\n",
    "       .withColumn(\"DISCHTIME\", col(\"DISCHTIME\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate length of stay in days\n",
    "df = df.withColumn(\"LENGTH_OF_STAY\", datediff(col(\"DISCHTIME\"), col(\"ADMITTIME\")))\n",
    "\n",
    "# Calculate number of previous admissions\n",
    "window_spec = Window.partitionBy(\"SUBJECT_ID\").orderBy(\"ADMITTIME\")\n",
    "df = df.withColumn(\"NUM_PREV_ADMISSIONS\", row_number().over(window_spec) - 1)\n",
    "\n",
    "# Create the READMISSION label (1 if readmitted within 30 days, 0 otherwise)\n",
    "df = df.withColumn(\"READMISSION\", when(datediff(lag(\"ADMITTIME\").over(window_spec), col(\"DISCHTIME\")) <= 30, 1).otherwise(0))\n",
    "\n",
    "# Drop rows with null values (e.g., first admission for each patient)\n",
    "df = df.na.drop()\n",
    "\n",
    "# Show the prepared data\n",
    "df.select(\"SUBJECT_ID\", \"HADM_ID\", \"LENGTH_OF_STAY\", \"NUM_PREV_ADMISSIONS\", \"READMISSION\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c6f557-c0c1-469e-a9fe-38d1527ab44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|READMISSION|count|\n",
      "+-----------+-----+\n",
      "|          1|  693|\n",
      "|          0| 1360|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count the number of readmissions and non-readmissions\n",
    "readmission_counts = df.groupBy(\"READMISSION\").count()\n",
    "\n",
    "# Show the result\n",
    "readmission_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124a1a7-17b3-4546-ac23-beedc09934f4",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f91c983-cc5c-4daa-a6da-97576dd24274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|features  |READMISSION|\n",
      "+----------+-----------+\n",
      "|[15.0,2.0]|1          |\n",
      "|[7.0,2.0] |1          |\n",
      "|[0.0,1.0] |1          |\n",
      "|[0.0,3.0] |1          |\n",
      "|[2.0,3.0] |1          |\n",
      "|[1.0,1.0] |1          |\n",
      "|[10.0,1.0]|1          |\n",
      "|[1.0,1.0] |1          |\n",
      "|[46.0,8.0]|1          |\n",
      "|[7.0,1.0] |1          |\n",
      "|[9.0,2.0] |1          |\n",
      "|[1.0,4.0] |1          |\n",
      "|[9.0,3.0] |1          |\n",
      "|[19.0,6.0]|1          |\n",
      "|[12.0,2.0]|1          |\n",
      "|[4.0,0.0] |0          |\n",
      "|[16.0,4.0]|1          |\n",
      "|[2.0,1.0] |1          |\n",
      "|[5.0,2.0] |1          |\n",
      "|[36.0,2.0]|1          |\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = [\"LENGTH_OF_STAY\", \"NUM_PREV_ADMISSIONS\"]\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Show the DataFrame with features\n",
    "df.select(\"features\", \"READMISSION\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6750d0-a2df-4d99-adfe-b28612daa8b6",
   "metadata": {},
   "source": [
    "## Step 3: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95c3d2e-24b2-476e-acf5-41e577e3e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (70% training, 30% test)\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2640c2-d9f8-413b-99cf-d933d25bfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"READMISSION\")\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0935f6-0c28-4f60-a4ea-c9cf8f5550c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"READMISSION\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ccc5e5-7bf0-4b83-8230-61488f8a36d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_score = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "print(\"F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a9a83a-d3f6-44f1-8922-0e6756bacc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|READMISSION|count|\n",
      "+-----------+-----+\n",
      "|          1|  214|\n",
      "|          0|  397|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_stats = predictions.groupBy(\"READMISSION\").count()\n",
    "pred_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c3b11d-d1a1-4391-aece-1560624d4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|  397|\n",
      "|       1.0|  214|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_stats2 = predictions.groupBy(\"prediction\").count()\n",
    "pred_stats2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c966179c-5f6b-4abb-b555-952c66c673de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Linear Regression model to a file\n",
    "model_save_path = \"hdfs://namenode:9000/models/readmission_risk_prediction_lr_model\"\n",
    "lr_model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc100c0a-b34c-4632-9069-fc0ebc6e8edb",
   "metadata": {},
   "source": [
    "# Part II. Length-of-Stay (LOS) regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b3a7d45-3fc5-4da3-affe-dd1b320a9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631045a7-3b78-45fe-8226-36c63c62647f",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da6e94e-6f68-49b8-a4b0-cd259cebe0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ADMISSIONS.csv file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"hdfs://namenode:9000/mimic-iii/ADMISSIONS.csv\")\n",
    "\n",
    "# Load PATIENTS.csv and rename ROW_ID to PATIENT_ROW_ID\n",
    "patients_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"hdfs://namenode:9000/mimic-iii/PATIENTS.csv\") \\\n",
    "    .withColumnRenamed(\"ROW_ID\", \"PATIENT_ROW_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49853f6-d58a-466c-bc1a-a85d87b6a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+--------------+\n",
      "|SUBJECT_ID|          ADMITTIME|          DISCHTIME|LENGTH_OF_STAY|\n",
      "+----------+-------------------+-------------------+--------------+\n",
      "|       109|2142-08-28 19:48:00|2142-08-30 15:20:00|             2|\n",
      "|       111|2144-07-01 04:12:00|2144-07-01 14:55:00|             0|\n",
      "|       304|2141-05-18 17:21:00|2141-05-19 01:45:00|             1|\n",
      "|       353|2153-06-27 20:15:00|2153-07-07 10:30:00|            10|\n",
      "|       502|2143-10-23 21:05:00|2143-11-04 17:28:00|            12|\n",
      "|       505|2154-08-23 14:01:00|2154-08-29 11:10:00|             6|\n",
      "|       188|2161-11-01 17:48:00|2162-01-17 05:50:00|            77|\n",
      "|       250|2188-11-12 09:22:00|2188-11-22 12:00:00|            10|\n",
      "|       275|2170-10-06 03:09:00|2170-10-19 15:35:00|            13|\n",
      "|       907|2163-10-01 22:51:00|2163-10-02 15:53:00|             1|\n",
      "|       546|2127-04-01 16:33:00|2127-04-07 17:37:00|             6|\n",
      "|       433|2164-08-13 17:22:00|2164-08-17 12:00:00|             4|\n",
      "|       975|2142-05-15 16:16:00|2142-06-21 14:38:00|            37|\n",
      "|       985|2119-04-30 16:17:00|2119-05-06 12:50:00|             6|\n",
      "|      1006|2159-08-20 22:15:00|2159-08-29 14:43:00|             9|\n",
      "|       747|2147-03-07 11:51:00|2147-03-07 21:02:00|             0|\n",
      "|       773|2109-02-18 06:04:00|2109-03-17 20:25:00|            27|\n",
      "|      1197|2197-11-06 19:06:00|2197-11-24 12:00:00|            18|\n",
      "|      1141|2192-06-29 20:01:00|2192-07-02 14:15:00|             3|\n",
      "|      1401|2185-09-09 16:25:00|2185-09-17 12:00:00|             8|\n",
      "+----------+-------------------+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert ADMITTIME and DISCHTIME to timestamps\n",
    "df = df.withColumn(\"ADMITTIME\", col(\"ADMITTIME\").cast(\"timestamp\")) \\\n",
    "       .withColumn(\"DISCHTIME\", col(\"DISCHTIME\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate Length of Stay (LOS) in days\n",
    "df = df.withColumn(\"LENGTH_OF_STAY\", datediff(col(\"DISCHTIME\"), col(\"ADMITTIME\")))\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Show the prepared data\n",
    "df.select(\"SUBJECT_ID\", \"ADMITTIME\", \"DISCHTIME\", \"LENGTH_OF_STAY\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cafbdf0-6ba7-45b4-80b9-036c28c40e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join ADMISSIONS with PATIENTS on SUBJECT_ID\n",
    "df = df.join(patients_df, on=\"SUBJECT_ID\", how=\"left\")\n",
    "\n",
    "# Convert ADMITTIME and DOB to timestamps\n",
    "df = df.withColumn(\"ADMITTIME\", col(\"ADMITTIME\").cast(\"timestamp\")) \\\n",
    "       .withColumn(\"DOB\", col(\"DOB\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate Length of Stay (LOS) in days\n",
    "df = df.withColumn(\"LENGTH_OF_STAY\", datediff(col(\"DISCHTIME\"), col(\"ADMITTIME\")))\n",
    "\n",
    "# Calculate age at admission\n",
    "df = df.withColumn(\"AGE\", datediff(col(\"ADMITTIME\"), col(\"DOB\")) / 365)\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1715a-0623-43f5-bd42-7e5ffbcfb767",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9434b400-31eb-4b63-ac6b-07d9493a8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"INSURANCE\", \"LANGUAGE\", \"RELIGION\", \"MARITAL_STATUS\", \"ETHNICITY\", \"GENDER\"]\n",
    "\n",
    "# Check and drop columns with only one distinct value\n",
    "for col_name in categorical_columns:\n",
    "    distinct_count = df.select(col_name).distinct().count()\n",
    "    if distinct_count < 2:\n",
    "        print(f\"Dropping column '{col_name}' because it has only {distinct_count} distinct value(s).\")\n",
    "        categorical_columns.remove(col_name)\n",
    "\n",
    "# Step 1: StringIndexer\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\").fit(df) for col in categorical_columns]\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "# Step 2: OneHotEncoder\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n",
    "for encoder in encoders:\n",
    "    df = encoder.fit(df).transform(df)\n",
    "\n",
    "# Step 3: Assemble features\n",
    "feature_columns = [\"AGE\", \"HOSPITAL_EXPIRE_FLAG\"] + [col + \"_encoded\" for col in categorical_columns]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4ccc8-d2fc-4625-8b07-91faed8a7ed5",
   "metadata": {},
   "source": [
    "## Step 3: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8948292c-0549-4bdb-809f-6a857ecb165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f80e83ff-5f20-4b42-9bb9-65df6afa957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"LENGTH_OF_STAY\")\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0e91d4b-e92b-4dd9-bb07-8e6257e51164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6187fde-b6eb-4352-b808-072773f7d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 10.497199129596577\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"LENGTH_OF_STAY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1dfb6b4-3d86-421b-b74e-30333696f582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 110.19118956640315\n"
     ]
    }
   ],
   "source": [
    "evaluator.setMetricName(\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72331867-6b21-4b8b-a057-7465058e0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R²): -0.13062202346993468\n"
     ]
    }
   ],
   "source": [
    "evaluator.setMetricName(\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R-squared (R²):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89b538d4-68b6-4ee3-a421-31fd52091697",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1219.save.\n: java.io.IOException: Path hdfs://namenode:9000/models/length_of_stay_lr_model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the trained Linear Regression model to a file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/models/length_of_stay_lr_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/util.py:262\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/util.py:213\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1219.save.\n: java.io.IOException: Path hdfs://namenode:9000/models/length_of_stay_lr_model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained Linear Regression model to a file\n",
    "model_save_path = \"hdfs://namenode:9000/models/length_of_stay_lr_model\"\n",
    "lr_model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fdf80-16b3-4c3f-ba4b-e6861e44fb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
